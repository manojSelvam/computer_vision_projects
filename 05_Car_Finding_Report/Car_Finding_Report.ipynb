{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle Detection and Tracking\n",
    "___\n",
    "The goal of this project is to detect vehicles in a video stream and draw bounding boxes around the detected vehicles. A classifier is trained to detect vehicles and non-vehicles. The dataset provided by udacity is used. \n",
    "\n",
    "* __Feature Extraction and Training:__ Different types of signatures are extracted from the image to train the classifier.\n",
    "\n",
    "* __Detect vehicles using sliding windows:__ Using the trained classifier to detect vehicles in a given frame using a sliding window approach. \n",
    "\n",
    "* __Merge Multiple Detections:__ Sliding window approach produces multiple detections for the same object due to the overlap of the windows. A heatmap is used to form a  union of multiple detections within a region.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration:\n",
    "\n",
    "The dataset consists of car images (8792) and non-car images (8968). The classes are well balanced in the dataset,\n",
    "hence we do not have to worry about the shape of the dataset.\n",
    "\n",
    "#### Cars\n",
    "<p align=\"center\">\n",
    "  <img src=\"car_tracking_report_img/cars.png\" alt=\"\"/>\n",
    "</p>\n",
    "\n",
    "#### Non Cars\n",
    "<p align=\"center\">\n",
    "  <img src=\"car_tracking_report_img/non_cars.png\" alt=\"\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction:\n",
    "\n",
    "The images in the dataset are RGB 64x64 pixels in dimensions. The following features are extracted from the image.\n",
    "\n",
    "* __Histogram of Oriented Gradients (HOG)__\n",
    "Different colour spaces and HOG parameters were explored before settling on the following HOG parameters.\n",
    "\n",
    "```\n",
    "orientations=10 | pixels_per_cell=(8, 8) | cells_per_block=2\n",
    "```\n",
    "\n",
    "##### Images\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"car_tracking_report_img/img.png\" alt=\"\"/>\n",
    "</p>\n",
    "\n",
    "##### Hog feature\n",
    "<p align=\"center\">\n",
    "  <img src=\"car_tracking_report_img/hog.png\" alt=\"\"/>\n",
    "</p>\n",
    "\n",
    "The Image format used here is YUV format.The HOG features are extracted from each channel individually and concatenated together. the hog feature extraction from `sklearn` is used to detect hog features. \n",
    "\n",
    "```python\n",
    "yuv = cvt_image_colour_space(img,'YUV')\n",
    "for channel in range(3):\n",
    "features.append(get_hog_features(yuv[:,:,channel],vis=False,orient=10,pix_per_cell=8))\n",
    "```\n",
    "\n",
    "Increasing the `pixels_per_cell` improved the speed of feature extraction but does not improve classifier accuracy. `get_hog_features` method is used to extract the features\n",
    "\n",
    "* __Colour Channel Histogram__\n",
    "\n",
    "Histogram of individual channels are stringed together.\n",
    "```\n",
    "color_hist(img, nbins=32, bins_range=(0, 256))\n",
    "```\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"car_tracking_report_img/image_histogram.png\" alt=\"\"/>\n",
    "</p>\n",
    "\n",
    "* __Spatial Binning__\n",
    "\n",
    "To capture the locality of pixels the images are reduced to 32x32 px and flattened to an array .\n",
    "```\n",
    "features = cv2.resize(feature_image, size).ravel() \n",
    "```\n",
    "\n",
    "\n",
    "`get_features()` method is used to extract the features from an image. The method does the following:\n",
    " \n",
    " * HOG features\n",
    " * Colour Histogram features\n",
    " * Spatial Binning features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "\n",
    "\n",
    "\n",
    "A `LinearSVC` was used as the classifier. Features are extracted from every image in the dataset.The data set is normalised using the sklearn's StandardScalar method.Once the features are normalised the dataset is split into training and test dataset using `train_test_split`.  \n",
    "\n",
    "`The implementation is in the notebook attached `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding windows\n",
    "\n",
    "The classifier is trained with images od size 64x64 pixels. To be able to detect the cars in a given image a window's of different sizes are moved across the region of interest within the image.A vehicle present in the scene will get smaller will the distance from the car. The large sized windows are used to pickup vehicles closer to the car. the smaller sized windows are used to pickup vehicles farther away. \n",
    "\n",
    "If the window sizes are not always 64x64 pixels, the region bounded by the window would resize to 64x64 pixels. As an implementation detail, instead of changing the window size we resize images to different scales and scan it with the same window producing the same results.\n",
    "\n",
    "`find_cars(image,scale,y_scale,window_width=64)` method is used to scale the input image down and slide a window across the image and returns all the detections within the image. \n",
    "\n",
    "The sliding search is so that the stride is no less than one-third of the window. Implementation could be found in `find_cars` method. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"car_tracking_report_img/sliding_windows.png\" alt=\"\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging detections\n",
    "\n",
    "Since the image is being scanned multiple times with windows of different sizes. It is possible that same vehicle is picked up multiple times with different window sizes. A heatmap is formed using the intersecting regions of detections.   \n",
    "\n",
    "```python\n",
    "for coordinate in coordinates:\n",
    "  #(x1, y1, x2, y2)\n",
    "  x1 = coordinate[0]\n",
    "  y1 = coordinate[1]\n",
    "  x2 = coordinate[2]\n",
    "  y2 = coordinate[3]\n",
    "            \n",
    "  heatmap[y1:y2, x1:x2] += 1\n",
    "```\n",
    "\n",
    "The heat map is thresholded and the label function from the `label` from `scipy.ndimage.measurements` is used to find groupings and bounding boxes for the groups. The implementation is in `merge_detections` method.\n",
    "\n",
    "###### Result from pipeline\n",
    "<p align=\"center\">\n",
    "  <img src=\"car_tracking_report_img/result.png\" alt=\"\"/>\n",
    "</p>\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the above stages are combined in the `pipeline` method to process the videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above individual stages are used in the pipeline to detect cars in the video. Since the video is temporal in nature.In order to have better detections on the video stream, a cache of the past detections is used to form the heat map and thresholded accordingly. Using last N detections to form  teh heatmap helps reduce false positives. \n",
    "\n",
    "```python\n",
    "detection_history = deque(maxlen=20) # used to cache the previous detections\n",
    "  \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The left video stream is all detections on the frame and the right video stream is merged detections of multiple windows.\n",
    "___\n",
    "\n",
    "<video controls src=\"car_tracking_report_img/car_detection_report_video.mp4\" width=900/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "This implementation is very crude in terms of performance and in the way bounding boxes are drawn.A stepped up of the system could be achieved by using low-level language-based implementation. A combination of the techniques used here along with a conv- net to detect objects would improve the confidence of the detections.\n",
    "\n",
    "* The hog feature extraction for sliding windows is very slow. This can be optimised by extracting the features and once and looking it up. \n",
    "* The algorithm is only as good as the data that is fed into it. If the input image is noisy the possibility of having false positives is large.\n",
    "* This pipeline does not cater for different weather conditions (rain and low light).  \n",
    "* A method to automatically detect vanishing points could be implemented to select the region of interest.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
